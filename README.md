# fedstellar-
Project Overview:
This project focuses on analyzing the performance of decentralized federated learning nodes to evaluate model accuracy, training efficiency, and convergence consistency. The objective is to study how distributed nodes collaboratively train models without centralizing data and to measure performance variations across nodes.

The project applies Python-based data analysis techniques to examine training logs, accuracy trends, and communication efficiency in a decentralized federated learning setup.

Problem Statement:
Federated learning enables distributed model training without sharing raw data. However, decentralized architectures introduce challenges such as inconsistent convergence, varying node performance, and communication overhead. This project evaluates these aspects through systematic performance analysis.

Technologies Used :
Python
Pandas
NumPy
Matplotlib
Seaborn


Project Workflow:

Data Collection:
Collected training metrics from decentralized federated learning nodes including accuracy, loss, and training time.

Data Preprocessing:
Cleaned and structured performance logs
Handled missing values and inconsistencies
Standardized metrics for comparison across nodes

Performance Analysis:
Analyzed node-level accuracy trends
Evaluated convergence speed across training rounds
Compared training efficiency among decentralized nodes
Measured variance in model performance

Visualization:
Plotted accuracy and loss curves
Visualized convergence patterns
Compared node-wise performance distributions

Key Outcomes :
Identified performance variability among decentralized nodes
Evaluated convergence consistency across training rounds
Derived insights into training efficiency and communication behavior
